{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heteroscedasticity in Regression\n",
    "\n",
    "When your regression variance is non-stationary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Gaussian Process model for Heteroscedasticity\n",
    "\n",
    "A common phenomenon when working on continuous regression problems is the non-constant residual variance, also known as heteroscedasticity. Although predicting the mean via MSE-minimisation is often sufficient and more pragmatic, a proper treatment of the variance can be helpful at times."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem definition\n",
    "\n",
    "At the heart of non-constant variance models, lies the assumption of some functional relation between the input data and the variance of the target variable. Presuming also a Gaussian target variable, we can construct the following probabilistic setup:\n",
    "\n",
    "$$ y \\sim N(m(X), \\sigma^{2}(X)) $$\n",
    "\n",
    "Put plainly, given some input data, the corresponding target should be Gaussian with mean and variance being arbitrary functions of our inputs. Since our today is on the variance, let us simplify things a little with a zero-mean function.\n",
    "\n",
    "In simple terms, we now don’t expect that a single model would best describe our target function anymore. Instead, a — possibly infinitely large — set of models is considered and our goal is to place a probability distribution (a.k.a. posterior distribution) on this set such that those models that best describe the data (a.k.a. likelihood) given the assumptions we made (a.k.a. prior distributions) are the most likely ones.\n",
    "\n",
    "This is usually done in weight space by defining our set of models in an implicit manner via the sets of parameters that describe the models’ behaviour — probably the most famous example in Machine Learning are Bayesian Neural Networks. Another, more abstract approach is to directly work in function space, i.e. we now explicitly look for the most likely functions without requiring parameters to describe them in the first place. Since we are working in the Bayesian domain, this also means that prior and posterior distributions aren’t put over parameters anymore but also directly over functions. One of the most iconic frameworks for such modelling is Gaussian Process (GP) regression.\n",
    "\n",
    "### The Model:\n",
    "\n",
    "Our goal will be to model the variance of the target variable through a Gaussian Process, which looks as follows:\n",
    "\n",
    "$$ y \\sim N( 0,f^{exp}(X)) $$\n",
    "\n",
    "$$ f(.) \\sim GP(0,k(.,.)+v*\\delta_{ij}), f^{exp}(X) = exp(f(X)) $$\n",
    "\n",
    "This implies that the logarithm of our variance function is a GP — we need to squash the raw GP through an exponential to ensure that the variance will always be greater than zero. Any other function that maps the real line to the positive reals will do here but the exponential arguably the most popular one. The above also implies that the GP is actually a latent component of our model that we only observe indirectly from the data we collect. Finally, we presumed additional noise on the GP kernel via the delta summand which makes the model more stable in practice.\n",
    "\n",
    "We can derive the posterior distribution derived via Bayes theorem as follows:\n",
    "\n",
    "$$ p(f|X,y) = N(y|0,f^{exp}(X)) * GP(f|0,k(.,.)+v^{2}*\\delta_{ij}) / p(y|X) $$ \n",
    "\n",
    "While it is possible to derive the left hand site in closed form for some basic GP models, we cannot do so in our case. Instead we will apply Laplace Approximation and approximate it through a synthetic multivariate normal:\n",
    "\n",
    "$$ p(f|X,y) \\approx N(f| \\hat{f},A) $$\n",
    "\n",
    "In summary, the mean parameter of our approximation should match the mode of the posterior, while it's covariance matrix is the negative inverse of the Hessian matrix of our log-likelihood function:\n",
    "\n",
    "$$ \\hat{f} = argmax_{f} log N(y|0,f^{exp}(X))  + log GP(f|0,k(.,.)+v^{2}*\\delta_{ij})$$\n",
    "\n",
    "$$ A^{-1} = - \\nabla^{2} log N(y|0,f^{exp}(X)) $$\n",
    "\n",
    "To find the approximate mean and optimal kernel hyper-parameters for some example data later on, we will plug in the whole loss into an automatic differentiation package and let the computer do the rest. For the covariance matrix of our approximation, we need to actually calculate the Hessian matrix. A common simplification for GP models is the assumption of independent observations of the target variable given a realisation of the latent GP:\n",
    "\n",
    "$$ p(y|f) = \\prod_{i=1}^{N} p(y_{i}|f) $$\n",
    "\n",
    "This allows us to simplify the Hessian matrix to be zero everywhere, except for its diagonal which is the second-order derivative of the log-likelihood function with respect to the GP:\n",
    "\n",
    "$$ H_{ii} = \\displaystyle \\frac{\\partial^{2} log p(y_{i}|f_{i})}{\\partial^{2} f_{i}} $$\n",
    "\n",
    "The right-hand side can be derived by differentiating the standard Gaussian log-likelihood twice with respect to the variance while accounting for our exponential transform:\n",
    "\n",
    "$$ \\displaystyle \\frac{\\partial^{2} log p(y_{i}|f_{i})}{\\partial^{2} f_{i}} = -0.5  \\frac{y_{i}^{2}}{exp(f_{i})} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
