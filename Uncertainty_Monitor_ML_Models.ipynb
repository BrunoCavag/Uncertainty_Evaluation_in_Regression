{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Uncertainty to monitor ML Models\n",
    "\n",
    "A crucial to retrain your model in production is monitoring: how do yo know when it is time ti retrain? when you can no longer trusts its predictions? \n",
    "\n",
    "*In this work, we use non-parametric bootstrapped uncertainty estimates and SHAP values to provide explainable uncertainty estimation as a technique that aims to monitor the deterioration of machine learning models in deployment environments, as well as determine the source of model deterioration when target labels are not available.*\n",
    "*Classical methods are purely aimed at detecting distribution shift, which can lead to false positives in the sense that the model has not deteriorated despite a shift in the data distribution.*\n",
    "\n",
    " The basic idea is to estimate and combine several sources of variation in our ML predictions, and create prediction intervals from the combination of all these sources.\n",
    "\n",
    "Firstly we want to estimate how much the model depends on specific samples of our training set. We achieve this by sampling parts of the dataset, fitting the model on each of them in turn, and measuring how different the predictions of the resulting models are. Secondly, our models might have an inherent bias, meaning that it will never be able to fully approximate the underlying data distribution, no matter how much data we throw at it. Lastly we want to estimate the noise that the model inherently has, again no matter how much data we throw at it. \n",
    "\n",
    "We can approximate all of these sources of noise using bootstrapping, and from these sources we can produce very accurate prediction intervals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting the source of deterioration\n",
    "\n",
    "One thing is to detect when a model is deteriorating, but sometimes we might want to know **how** is deteriorating.This could be due to the shift in one or more variables, the knowledge of which are the sources of a model deterioration might be useful in its own right. \n",
    "\n",
    "To account for the reasons of model deterioration, we fitted a separate model on the uncertainty values (the inputs of this model is the shifted feature values, and the outputs are the estimates uncertainties). We proceeded to compute SHAP values of this separate model, which shows which features of the datasets contributes the most to an increased uncertainty.\n",
    "\n",
    "To test this approach, and compare it to competing methods, we took one of our datasets and shifted two features, which are the most correlated with the target variable, **GrLivArea** and **TotalBsmtSF** as well as introducing a random variable and shifting that as well. We thus want the model to identify the first two features, but disregard the random one, as it is being shifted does not affect the model performance at all. Here are the results:\n",
    "\n",
    "![](https://saattrupdan.github.io/img/uncertainty-shap.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/MachineLearning_Training/Conformal_Estimation/conformal/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from doubt import Boot\n",
    "import numpy as np\n",
    "\n",
    "# Generate normal-distributed random data\n",
    "x1 = np.random.normal(1, 0.1, size=10000)\n",
    "x2 = np.random.normal(1, 0.1, size=10000)\n",
    "x3 = np.random.normal(1, 0.1, size=10000)\n",
    "\n",
    "# Create a synthetic dataset with the random data, of shape (1, 3)\n",
    "X = np.array([x1, x2, x3]).T\n",
    "\n",
    "# Create out-of-distribution data by shifting the first feature by 5\n",
    "X_ood = np.array([x1 + 5, x2, x3]).T\n",
    "\n",
    "# Create the target variable, which depends non-linearly on `x1`, linearly on `x2`, and does not depend on `x3` at all\n",
    "y = x1 ** 2 + x2 + np.random.normal(0, 0.01, 10000)\n",
    "\n",
    "# Create linear regression model with uncertainty estimation support, using our `Boot` wrapper class\n",
    "clf = Boot(LinearRegression())\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01, -0.  , -0.  ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute predictions along with prediction intervals on the out-of-distribution data\n",
    "preds, intervals = clf.predict(X_ood, uncertainty=0.05)\n",
    "\n",
    "# Compute the uncertainty, being the width of the prediction intervals\n",
    "unc = intervals[:, 1] - intervals[:, 0]\n",
    "\n",
    "# As for explaining where the uncertainty comes from, we fit a new linear regression model\n",
    "# on the out-of-distribution data, which attempts to predict the uncertainties\n",
    "m = LinearRegression().fit(X_ood, unc)\n",
    "\n",
    "# Print out the coefficients of the second model, which corresponds to the SHAP values.\n",
    "# We see that it puts no importance on any of the variables, as they are merely random\n",
    "np.round(m.coef_, decimals=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('conformal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd4f7231d637f047e5c76f93b07ffb507bf232d97b7362ed7276b0a7d26c050b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
